# Spare-it Documents

- `.\best-result`: This folder contains the current best results metrics.
    - **Confusion Matrix**: This is a table used to evaluate the performance of a classification model. It shows the counts of actual versus predicted classifications, helping us identify true positives, false positives, true negatives, and false negatives.
    - **F1 Curve**: The F1 curve displays the F1 score, which is the harmonic mean of precision and recall, across different confidence thresholds. It helps evaluate the balance between precision and recall.
    - **Labels Summary**: This document provides a summary of the labels, including the count of true positive, false positive, and false negative predictions for each class.
    - **Precision Confidence Curve**: This curve shows how precision varies with changing confidence thresholds. Precision indicates the proportion of positive predictions that are correct.
    - **Precision Recall Curve**: This curve displays the trade-off between precision and recall for different threshold values. It helps understand the balance between precision and recall.
    - **Recall Confidence Curve**: The recall confidence curve shows how recall varies with changing confidence thresholds. Recall indicates the proportion of actual positives that are correctly identified.
    - **Loss and Precision**: This document displays the loss and precision values throughout the training process, helping us understand how the model's performance improved over time.

- `Spare-it-Final.pdf`, `Spare-it-Midterm.pdf`: The final and midterm presentation slides