{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d9d616-c302-485e-845c-eed2f23fcf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, utils\n",
    "from PIL import Image\n",
    "from typing import Any, Dict\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843b6851-36ea-4f23-b607-1924635739b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4f0af2-cb84-41dd-afff-2cbbae376bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset class for loading the images\n",
    "class WasteImageDataset(Dataset):\n",
    "    def __init__(self, directory, transform=None):\n",
    "        \"\"\"Initializes the dataset object\n",
    "        Args:\n",
    "            directory (str): Directory path of the images.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.directory = directory\n",
    "        self.transform = transform\n",
    "        self.images = [os.path.join(directory, x) for x in os.listdir(directory) if x.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the number of images in the dataset\"\"\"\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Fetches the image at index `idx` and applies transformations if any\"\"\"\n",
    "        img_path = self.images[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "# Transformations applied to each image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "dataset = WasteImageDataset('./processed_images_cropped', transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Create directory for output image\n",
    "os.makedirs('output_images_18', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860a532c-dbcb-49f1-8971-021382e8df02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to initialize model weights\n",
    "def weights_init(model):\n",
    "     \"\"\"Applies initial weights to certain layers in a model.\n",
    "    The weights are initialized to a normal distribution.\n",
    "    \"\"\"\n",
    "    classname = model.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(model.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(model.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(model.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25c6f53-7c2b-438c-bc17-e8ffa0609b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator model\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"Maps a latent space vector (z) to data-space.\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs: Dict[str, Any]):\n",
    "        \"\"\"Creates a new instance of Generator.\n",
    "\n",
    "        Args:\n",
    "            **kwargs: An optional set of key/value pair arguments:\n",
    "                * latent_vector_size: The size of the Normally-distributed\n",
    "                latent-vector input. Defaults to 100.\n",
    "                * num_features: The feature size of the output data. Defaults\n",
    "                to 64 (e.g. 64x64 images).\n",
    "                * num_channels: The number of channels of the output data.\n",
    "                Defaults to 3 (e.g. RGB images).\n",
    "        \"\"\"\n",
    "\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.latent_vector_size = kwargs.get('latent_vector_size', 100)\n",
    "        self.num_features = kwargs.get('num_features', 64)\n",
    "        self.num_channels = kwargs.get('num_channels', 3)\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            # layer-1 100(1x1) -> 512(4x4)\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=self.latent_vector_size,\n",
    "                out_channels=(self.num_features * 8),\n",
    "                kernel_size=4,\n",
    "                stride=1,\n",
    "                padding=0,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(num_features=self.num_features*8),\n",
    "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "\n",
    "            # layer-2  512x(4x4) -> 256x(8x8)\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=(self.num_features * 8),\n",
    "                out_channels=(self.num_features * 8),\n",
    "                kernel_size=4,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(num_features=self.num_features * 8),\n",
    "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "\n",
    "            # layer-3  512x(8x8) -> 512x(16x16)\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=(self.num_features * 8),\n",
    "                out_channels=(self.num_features * 8),\n",
    "                kernel_size=4,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(num_features=self.num_features * 8),\n",
    "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "\n",
    "            # layer-4  512x(16x16) -> 256x(32x32)\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=(self.num_features * 8),\n",
    "                out_channels=(self.num_features*4),\n",
    "                kernel_size=4,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(num_features=self.num_features*4),\n",
    "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "\n",
    "\n",
    "            # layer-5  256x(32x32) -> 128x(64x64)\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=(self.num_features * 4),\n",
    "                out_channels=(self.num_features*2),\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(num_features=self.num_features*2),\n",
    "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "\n",
    "            # layer-5  128x(64x64) -> 3x(64x64)\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=self.num_features*2,\n",
    "                out_channels=self.num_channels,\n",
    "                kernel_size=4,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    # forward propagation\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3584e2-7bb1-447b-9f4a-d3d50ee6f8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator model\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    A binary classification network that takes an image as input and outputs\n",
    "    a scalar probability that the input image is real (as opposed to fake).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs: Dict[str, Any]):\n",
    "        \"\"\"Creates a new instance of Discriminator.\n",
    "\n",
    "        Args:\n",
    "            **kwargs: An optional set of key/value pair arguments:\n",
    "                * num_features: The feature size of the input data. Defaults\n",
    "                to 64 (e.g. 64x64 images).\n",
    "                * num_channels: The number of channels of the input data.\n",
    "                Defaults to 3 (e.g. RGB images).\n",
    "        \"\"\"\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.num_features = kwargs.get('num_features', 64)\n",
    "        self.num_channels = kwargs.get('num_channels', 3)\n",
    "\n",
    "        # descriminator network\n",
    "        self.main = nn.Sequential(\n",
    "            # layer-1 3x(64x64) -> 64x(32x32)\n",
    "            nn.Conv2d(\n",
    "                in_channels=self.num_channels,\n",
    "                out_channels=self.num_features,\n",
    "                kernel_size=4,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "\n",
    "            # layer-2 64x(32x32) -> 128x(16x16)\n",
    "            nn.Conv2d(\n",
    "                in_channels=self.num_features,\n",
    "                out_channels=(self.num_features * 2),\n",
    "                kernel_size=4,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(num_features=self.num_features * 2),\n",
    "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "\n",
    "            # layer-3 128x(16x16) -> 256x(8x8)\n",
    "            nn.Conv2d(\n",
    "                in_channels=(self.num_features * 2),\n",
    "                out_channels=(self.num_features * 4),\n",
    "                kernel_size=4,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(num_features=self.num_features * 4),\n",
    "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "\n",
    "            # layer-4 256x(8x8) -> 512x(4x4)\n",
    "            nn.Conv2d(\n",
    "                in_channels=(self.num_features * 4),\n",
    "                out_channels=(self.num_features * 8),\n",
    "                kernel_size=4,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(num_features=self.num_features * 8),\n",
    "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
    "\n",
    "            #  layer-5 512x(4x4) -> 1x(1x1)\n",
    "            nn.Conv2d(\n",
    "                in_channels=(self.num_features * 8),\n",
    "                out_channels=1,\n",
    "                kernel_size=4,\n",
    "                stride=1,\n",
    "                padding=0,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3083284a-2deb-4b2a-99c6-e7fccce8afc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and setup the Generator and Discriminator\n",
    "netG = Generator(latent_vector_size=100, num_features=64, num_channels=3).to(device)\n",
    "netD = Discriminator(num_features=64, num_channels=3).to(device)\n",
    "\n",
    "# Initialize weights\n",
    "netG.apply(weights_init)\n",
    "netD.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d397cc20-f80d-451d-a48f-b059b0131811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rates for the Generator and Discriminator\n",
    "lr_generator = 0.0002\n",
    "lr_discriminator = 0.0002\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.BCELoss()\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr_generator, betas=(0.5, 0.999))\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr_discriminator, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c349ef03-dabd-4234-9e52-64f5955bc777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to show generated images\n",
    "def show_generated_img():\n",
    "    \"\"\"Generates and displays an image using the Generator\"\"\"\n",
    "    with torch.no_grad():\n",
    "        noise = torch.randn(1, 100, 1, 1, device=device)\n",
    "        fake = netG(noise).detach().cpu()\n",
    "    plt.imshow(np.transpose(utils.make_grid(fake, padding=2, normalize=True).cpu(),(1,2,0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3ba7b4-7837-4e18-8f39-fea360223666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables to store losses\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "\n",
    "# Main training loop\n",
    "num_epochs = 9000\n",
    "\n",
    "for epoch in range(num_epochs): \n",
    "    for i, real_images in enumerate(dataloader):\n",
    "        # Update Discriminator: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        netD.zero_grad()\n",
    "        real_data = real_images.to(device)\n",
    "        batch_size = real_data.size(0)\n",
    "        labels = torch.full((batch_size,), 1, dtype=torch.float, device=device)\n",
    "\n",
    "        # Forward pass real batch through D\n",
    "        output = netD(real_data).view(-1)\n",
    "        errD_real = criterion(output, labels)\n",
    "        errD_real.backward()\n",
    "\n",
    "        # Generate fake image batch with G\n",
    "        noise = torch.randn(batch_size, 100, 1, 1, device=device)\n",
    "        fake_images = netG(noise)\n",
    "        labels.fill_(0)\n",
    "\n",
    "        # Classify all fake batch with D\n",
    "        output = netD(fake_images.detach()).view(-1)\n",
    "        errD_fake = criterion(output, labels)\n",
    "        errD_fake.backward()\n",
    "        errD = errD_real + errD_fake\n",
    "        optimizerD.step()\n",
    "\n",
    "        # Update G: maximize log(D(G(z)))\n",
    "        netG.zero_grad()\n",
    "        labels.fill_(1)  # Fake labels are real for generator cost\n",
    "        output = netD(fake_images).view(-1)\n",
    "        errG = criterion(output, labels)\n",
    "        errG.backward()\n",
    "        optimizerG.step()\n",
    "\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            print(f'[{epoch}/{num_epochs}][{i}/{len(dataloader)}] Loss_D: {errD.item()} Loss_G: {errG.item()}')\n",
    "\n",
    "        if i % 250 == 0:  \n",
    "            show_generated_img()\n",
    "\n",
    "        if (i % 500 == 0) or ((epoch == num_epochs - 1) and (i == len(dataloader) - 1)):\n",
    "            with torch.no_grad():\n",
    "                fake = netG(noise).detach().cpu()\n",
    "            utils.save_image(fake, f'output_images_18/fake_samples_epoch_{epoch}_iter_{i}.png', normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a80bd1-c586-449e-88fc-96944687a3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the training losses\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(G_losses,label=\"Generator Loss\")\n",
    "plt.plot(D_losses,label=\"Discriminator Loss\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
